{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Google Drice 掛載**"
      ],
      "metadata": {
        "id": "T4DDZWHpOLCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTdMTvB7FVe8",
        "outputId": "01d0b1ad-953a-420b-9ce6-65b9410c8fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**套件匯入**\n",
        "\n",
        "import 文字轉向量與預處理套件\n",
        "\n",
        "匯入自訓練模型(根據爬蟲下來的文本)"
      ],
      "metadata": {
        "id": "bGLKn3dwOQFq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDjP0XhiY9vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294824bb-438f-4032-cec3-f5b3827c629b"
      },
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models import FastText\n",
        "import pandas as pd \n",
        "import re ,nltk\n",
        "from nltk.tokenize import regexp_tokenize,sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))  \n",
        "fname = get_tmpfile(\"/content/drive/MyDrive/aws_聊天機器人/AWS_CORPUS_fasttext3.model\")\n",
        "model = FastText.load(fname)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  \n",
        "# import gensim.downloader\n",
        "# model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "讀取根據AWS Blog FQA Support 所爬下來的文本"
      ],
      "metadata": {
        "id": "jHT4dATpOl7c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FqPRs6vZDlU",
        "outputId": "a4de34a2-ab81-4457-8503-93a954b20dfd"
      },
      "source": [
        "#from nltk.corpus import stopwords\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')\n",
        "wtlem = WordNetLemmatizer()\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = set(stopwords.words('english'))  \n",
        "# print(word_tokenize(text))\n",
        "SUPPORT = pd.read_json(\"/content/drive/MyDrive/aws_聊天機器人/final_support_data.json\").dropna()\n",
        "BLOG = pd.read_json(\"/content/drive/MyDrive/aws_聊天機器人/aws_blog_data.json\").dropna()\n",
        "FQA = pd.read_json(\"/content/drive/MyDrive/aws_聊天機器人/Faq_corpus2.json\").dropna()\n",
        "\n",
        "stop_words.add(\"i\")\n",
        "drop_list = {\"about\",\"above\",\"after\",\"before\",\"after\",\"between\",\"be\",\"do\",\"for\",\"have\",\"can\",\"where\",\"which\",\"how\",\"what\",\"why\",\"who\",\"when\",\"because\",\"q\"}\n",
        "stop_words = set.difference(stop_words,drop_list)\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"it's\", 'each', \"don't\", 've', \"isn't\", 'or', 'hers', \"hasn't\", \"you're\", \"mightn't\", 'herself', 'too', \"didn't\", 'no', 'shan', 'yourself', 'd', 'in', 're', 'any', \"couldn't\", \"you'll\", 'doing', 'own', 'you', 'theirs', \"should've\", 'couldn', \"wasn't\", 'while', 'such', 'wouldn', 'does', 'it', \"haven't\", 'ourselves', 'were', 'that', 'this', 'being', 'once', 'on', 'now', 'of', \"that'll\", 'was', 'again', 'nor', 'than', \"doesn't\", 'hasn', 'whom', 'other', 'shouldn', \"mustn't\", 'they', 'aren', 'as', 'here', 'all', 'into', \"hadn't\", 'didn', 'up', 'don', 'ain', 'just', \"you've\", 'under', 'am', 'has', 'over', 'doesn', 'by', 'haven', 'mustn', 'ours', 'isn', 'from', \"won't\", 'needn', 'their', 'himself', 'will', \"aren't\", 'with', 'ma', 'wasn', 'o', 'me', 'had', 'yours', \"needn't\", 'your', 'did', 'those', 'out', 'then', 'there', 'against', 'through', \"shouldn't\", 'themselves', 'same', 'mightn', 'and', 'should', 'll', 'my', 'she', 'having', 'them', 'off', 'only', \"wouldn't\", 't', \"weren't\", 'are', 'these', 'won', 'some', 'to', 'most', 'so', 'until', 'very', 'hadn', 'the', 'few', 'his', 's', 'itself', 'further', 'below', 'y', 'not', 'he', 'its', 'is', 'both', 'more', 'we', \"you'd\", \"shan't\", 'weren', 'during', 'm', 'our', 'yourselves', 'at', 'down', 'an', 'but', 'her', \"she's\", 'myself', 'been', 'a', 'him', 'i', 'if'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文字預處理、刪除標點符號\n",
        "\n",
        "文字預處理包含:英文正則表達、刪除停用詞、字根還原"
      ],
      "metadata": {
        "id": "CXWkW3LsOwYg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_tCFvz1ZVCE"
      },
      "source": [
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"\n",
        "    import re\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)\n",
        "\n",
        "def preprocess_sent(new_sent):\n",
        "  new_sent = new_sent.lower() \n",
        "  sent_stayWord = re.sub(r'[^\\w\\s]','',new_sent)                #去除符號 數字資料及本身已經刪除 \n",
        "  corpus_tokenize = regexp_tokenize(sent_stayWord, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
        "  for i in range(len(corpus_tokenize)):                                             \n",
        "    corpus_tokenize[i] = wtlem.lemmatize(corpus_tokenize[i],'v')        #wordnet\n",
        "  # print(corpus_tokenize)\n",
        "  filtered_sentence = [w for w in corpus_tokenize if not w in stop_words]  #刪除停用詞\n",
        "  return filtered_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "預處理結果顯示"
      ],
      "metadata": {
        "id": "AXy0ErNVO5iH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VEts6a5-Og3",
        "outputId": "56ccbf75-6c8c-4811-ea51-6b63d16cbf5e"
      },
      "source": [
        "nltk.download('omw-1.4')\n",
        "preprocess_sent(\"i are good boy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'good', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "處理好的資料集合併"
      ],
      "metadata": {
        "id": "n7Cqa2hFPaB8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQHETTIcaCWW"
      },
      "source": [
        "all_Title = pd.concat([SUPPORT[['Title','url']],BLOG[['Title','url']]],axis=0).reset_index()\n",
        "get_vec_title =[]\n",
        "# for ti in all_Title['Title']:\n",
        "#   print(preprocess_sent(ti))\n",
        "all_Title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "進行Fasttext編碼"
      ],
      "metadata": {
        "id": "ILlDreGeF7ON"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruG5TLHpaa8f"
      },
      "source": [
        "'''\n",
        "def get_embedding_from_fast_text(corpus_title , model_): #包含preprocess\n",
        "  all_sent_vec =[]\n",
        "  for ti in corpus_title:\n",
        "    ti_pre = preprocess_sent(ti)\n",
        "    sent_vec = []\n",
        "    for i in ti_pre:\n",
        "      sent_vec.append(model_.wv[i]) \n",
        "      print(i)\n",
        "    all_sent_vec.append(sum(sent_vec)/len(ti_pre))\n",
        "  return all_sent_vec\n",
        "'''\n",
        "\n",
        "def get_embedding_from_single_sent(sent_,model_):\n",
        "  sent_vec = []\n",
        "  ti_pre = preprocess_sent(sent_)\n",
        "  for i in ti_pre:\n",
        "    # print(ti_pre)\n",
        "\n",
        "    sent_vec.append(model.wv[i]) \n",
        "  #print(sent_vec[0])\n",
        "  return (sum(sent_vec)/len(ti_pre))\n",
        "\n",
        "def get_embedding_from_df(df , model_):\n",
        "  vector = []\n",
        "  #print(i)\n",
        "  for i in df :\n",
        "    #print(i)\n",
        "    vector.append(get_embedding_from_single_sent(i,model_))\n",
        "  return vector\n",
        "\n",
        "\n",
        "all_title_embedding = get_embedding_from_df(all_Title['Title'],model)\n",
        "all_qa_embedding = get_embedding_from_df(FQA['question'],model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799jmk_uaifn"
      },
      "source": [
        "all_Title.insert(2,\"Embedding\",all_title_embedding)\n",
        "FQA.insert(2,\"Embedding\",all_qa_embedding)\n",
        "#all_title_embedding[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "儲存處理過後的結果"
      ],
      "metadata": {
        "id": "VxokmhkGRiT-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBX5pkUMtSBP"
      },
      "source": [
        "# pd.to_json()\n",
        "all_Title.to_json(\"/content/drive/MyDrive/aws_聊天機器人/For_demo/all_title3.json\")\n",
        "FQA.to_json(\"/content/drive/MyDrive/aws_聊天機器人/For_demo/FQA3.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以開始問問題囉!"
      ],
      "metadata": {
        "id": "RpKHr2aJRlvd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Ts8oCbnEyp",
        "outputId": "c4a1943d-c695-4bbc-a976-93af3478b30c"
      },
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from scipy.spatial.distance import cdist\n",
        "qa = \"How does AWS Auto Scaling discover what resources can scale?\"\n",
        "q = get_embedding_from_single_sent(qa , model).reshape(1,50)\n",
        "# cosine_similarity(x, y).sum()\n",
        "cos_list =[]\n",
        "cos_list_qa = [ ]\n",
        "for i in range(len(all_Title)-1) :\n",
        "  blog = all_Title['Embedding'].values[i].reshape(1,50)\n",
        "  FQ_A = FQA['Embedding'].values[i].reshape(1,50)\n",
        "  #print(cosine_similarity(q, a))\n",
        "  cos_list.append(cosine_similarity(q, blog))\n",
        "  cos_list_qa.append(cosine_similarity(q, FQ_A))\n",
        "\n",
        "sorted_cos_list  = sorted(cos_list)\n",
        "sorted_cos_list_qa = sorted(cos_list_qa)\n",
        "\n",
        "for i in range(1,3):\n",
        "  print(\"您的問題可能是 \\n\",FQA.question[cos_list_qa.index(sorted_cos_list_qa[-i])])\n",
        "  print(\"解答為  \\n A: \",remove_html_tags(FQA.answer[cos_list_qa.index(sorted_cos_list_qa[-i])]))\n",
        "  print(\"您可以參考此Blog的連結-1\",all_Title.Title[cos_list.index(sorted_cos_list[-i])])\n",
        "  print(all_Title.url[cos_list.index(sorted_cos_list[-i])])\n",
        "  print(\"您可以參考此Blog的連結-2\",all_Title.Title[cos_list.index(sorted_cos_list[-i-2])])\n",
        "  print(all_Title.url[cos_list.index(sorted_cos_list[-i-2])])\n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "\n",
        "# print(\"您的問題可能是 \\n\",FQA.question[cos_list_qa.index(max(cos_list_qa))])\n",
        "# print(\"解答為  \\n A: \",FQA.answer[cos_list_qa.index(max(cos_list_qa))])\n",
        "# print(\"您可以參考此Blog的連結-1\",all_Title.Title[cos_list.index(max(cos_list))])\n",
        "# print(all_Title.url[cos_list.index(max(cos_list))])\n",
        "# print(\"您可以參考此Blog的連結-2\",all_Title.Title[cos_list.index(max(cos_list))])\n",
        "# print(all_Title.url[cos_list.index(max(cos_list))])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "您的問題可能是 \n",
            " Q. How do I select an application stack within AWS Auto Scaling?\n",
            "解答為  \n",
            " A:  You can either select an AWS CloudFormation stack or select resources based on common resource tag(s). Please note that currently, ECS services cannot be discovered using tags. \n",
            "\n",
            "\n",
            " Show less\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "您可以參考此Blog的連結-1 How do I resolve issues with DynamoDB auto scaling?\n",
            "https://aws.amazon.com//premiumsupport/knowledge-center/dynamodb-auto-scaling/\n",
            "您可以參考此Blog的連結-2 How do I delay Auto Scaling termination of unhealthy Amazon EC2 instances so I can troubleshoot them?\n",
            "https://aws.amazon.com//premiumsupport/knowledge-center/auto-scaling-delay-termination/\n",
            "------------------------------------------------------------------------------\n",
            "您的問題可能是 \n",
            " Q. What can I scale with AWS Auto Scaling?\n",
            "解答為  \n",
            " A:  You can use AWS Auto Scaling to setup scaling for the following resources in your application through a single, unified interface:\n",
            "\n",
            "Amazon EC2 Auto Scaling groups\n",
            "Amazon Elastic Container Service (ECS) services (currently ECS services cannot be discovered using resource tags) \n",
            "Amazon EC2 Spot Fleets\n",
            "Amazon DynamoDB throughput capacity\n",
            "Aurora replicas for Amazon Aurora \n",
            "\n",
            "\n",
            "\n",
            " Show less\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "您可以參考此Blog的連結-1 How do I ensure that Auto Scaling lifecycle actions complete successfully when I use AWS CodeDeploy?\n",
            "https://aws.amazon.com//premiumsupport/knowledge-center/lifecycle-actions-code-deploy/\n",
            "您可以參考此Blog的連結-2 How can I configure Amazon ECS Service Auto Scaling on Fargate? \n",
            "https://aws.amazon.com//premiumsupport/knowledge-center/ecs-fargate-service-auto-scaling/\n",
            "------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdi2RhR4mgO3",
        "outputId": "a5637843-54d0-49e7-b4bf-c6a76094f8f1"
      },
      "source": [
        "\n",
        "# print(\"q 為 : \",str(all_Title.Title[ind]))\n",
        "print(\"問題為 : \" , qa)\n",
        "print(\"index 為 : \" ,str(cos_list.index(max(cos_list))))\n",
        "print(\"cosine 為 :\" , str(max(cos_list)))\n",
        "# print(\"兩者之間的cosine : \", str(cos_list[ind]))\n",
        "print(\"Bot 回傳的 :\", str(all_Title.Title[cos_list.index(max(cos_list))]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "問題為 :  How does AWS Auto Scaling discover what resources can scale?\n",
            "index 為 :  92\n",
            "cosine 為 : [[0.9077897]]\n",
            "Bot 回傳的 : How do I ensure that Auto Scaling lifecycle actions complete successfully when I use AWS CodeDeploy?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "XLT3FFKzoTLh",
        "outputId": "6a4fd645-67bc-4c3e-a6a1-7553b900054b"
      },
      "source": [
        "remove_html_tags(FQA.answer[cos_list_qa.index(sorted_cos_list_qa[-2])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You can use AWS Auto Scaling to setup scaling for the following resources in your application through a single, unified interface:\\n\\nAmazon EC2 Auto Scaling groups\\nAmazon Elastic Container Service (ECS) services (currently ECS services cannot be discovered using resource tags) \\nAmazon EC2 Spot Fleets\\nAmazon DynamoDB throughput capacity\\nAurora replicas for Amazon Aurora \\n\\n\\n\\n Show less\\n\\n\\n\\n\\n\\n\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HTbTAWXaNkFk"
      }
    }
  ]
}